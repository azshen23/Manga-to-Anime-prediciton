{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>['Comedy', 'Ecchi', 'Harem', 'Otaku Culture', ...</td>\n",
       "      <td>Tomoya Aki, an otaku, has been obsessed with c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18048</th>\n",
       "      <td>['Kids']</td>\n",
       "      <td>An educational anime part of the \"Save our mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>['Adventure', 'Fantasy']</td>\n",
       "      <td>The story takes place in Lingshan, where a gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20971</th>\n",
       "      <td>['Military']</td>\n",
       "      <td>Monkeys battle polar bears in air combat.  Sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10303</th>\n",
       "      <td>['Action', 'Adventure', 'Sci-Fi']</td>\n",
       "      <td>A classic sci-fi TV anime. The adventures of s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  genres  \\\n",
       "401    ['Comedy', 'Ecchi', 'Harem', 'Otaku Culture', ...   \n",
       "18048                                           ['Kids']   \n",
       "5632                            ['Adventure', 'Fantasy']   \n",
       "20971                                       ['Military']   \n",
       "10303                  ['Action', 'Adventure', 'Sci-Fi']   \n",
       "\n",
       "                                                synopsis  \n",
       "401    Tomoya Aki, an otaku, has been obsessed with c...  \n",
       "18048  An educational anime part of the \"Save our mot...  \n",
       "5632   The story takes place in Lingshan, where a gro...  \n",
       "20971  Monkeys battle polar bears in air combat.  Sho...  \n",
       "10303  A classic sci-fi TV anime. The adventures of s...  "
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_anime = pd.read_csv('./input/anime.csv')\n",
    "\n",
    "data_anime = data_anime.filter(['genres', 'synopsis'])\n",
    "\n",
    "data_anime = shuffle(data_anime, random_state = 23)\n",
    "\n",
    "data_anime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action' 'Adult Cast' 'Adventure' 'Anthropomorphic' 'Avant Garde'\n",
      " 'Award Winning' 'Boys Love' 'CGDCT' 'Childcare' 'Combat Sports' 'Comedy'\n",
      " 'Crossdressing' 'Delinquents' 'Detective' 'Drama' 'Ecchi' 'Educational'\n",
      " 'Erotica' 'Fantasy' 'Gag Humor' 'Girls Love' 'Gore' 'Gourmet' 'Harem'\n",
      " 'Hentai' 'High Stakes Game' 'Historical' 'Horror' 'Idols (Female)'\n",
      " 'Idols (Male)' 'Isekai' 'Iyashikei' 'Josei' 'Kids' 'Love Polygon'\n",
      " 'Magical Sex Shift' 'Mahou Shoujo' 'Martial Arts' 'Mecha' 'Medical'\n",
      " 'Military' 'Music' 'Mystery' 'Mythology' 'Organized Crime'\n",
      " 'Otaku Culture' 'Parody' 'Performing Arts' 'Pets' 'Psychological'\n",
      " 'Racing' 'Reincarnation' 'Reverse Harem' 'Romance' 'Romantic Subtext'\n",
      " 'Samurai' 'School' 'Sci-Fi' 'Seinen' 'Shoujo' 'Shounen' 'Showbiz'\n",
      " 'Slice of Life' 'Space' 'Sports' 'Strategy Game' 'Super Power'\n",
      " 'Supernatural' 'Survival' 'Suspense' 'Team Sports' 'Time Travel'\n",
      " 'Vampire' 'Video Game' 'Visual Arts' 'Workplace']\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "genres = [literal_eval(genre) for genre in data_anime['genres'].values]\n",
    "\n",
    "genre_encoder = MultiLabelBinarizer()\n",
    "genres_encoded = genre_encoder.fit_transform(genres)\n",
    "num_genres = len(genres_encoded[0])\n",
    "print(genre_encoder.classes_)\n",
    "print(genres_encoded[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 19209\n",
      "Test size: 4803\n"
     ]
    }
   ],
   "source": [
    "# Split our data into train and test sets\n",
    "train_size = int(len(data_anime) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data_anime) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/_api/v2/version/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# Split our labels into train and test sets\n",
    "train_genres = genres_encoded[:train_size]\n",
    "test_genres = genres_encoded[train_size:]\n",
    "\n",
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "  def __init__(self, vocab_size):\n",
    "    self._vocab_size = vocab_size\n",
    "    self._tokenizer = None\n",
    "  \n",
    "  def create_tokenizer(self, text_list):\n",
    "    tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "    tokenizer.fit_on_texts(text_list)\n",
    "    self._tokenizer = tokenizer\n",
    "\n",
    "  def transform_text(self, text_list):\n",
    "    text_matrix = self._tokenizer.texts_to_matrix(text_list)\n",
    "    return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_phrase(strings, phrase):\n",
    "    modified_strings = []\n",
    "    for string in strings:\n",
    "        modified_string = string.replace(phrase, \"\")\n",
    "        modified_strings.append(modified_string)\n",
    "    return modified_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab from training corpus\n",
    "from preprocess import TextPreprocessor\n",
    "\n",
    "VOCAB_SIZE=900 # This is a hyperparameter, try out different values for your dataset\n",
    "\n",
    "train_qs = data_anime['synopsis'].values[:train_size]\n",
    "\n",
    "test_qs = data_anime['synopsis'].values[train_size:]\n",
    "\n",
    "train_qs = [str(element) for element in train_qs]\n",
    "test_qs = [str(element) for element in test_qs]\n",
    "\n",
    "train_qs = remove_phrase(train_qs, \"[Written by MAL Rewrite]\")\n",
    "test_qs = remove_phrase(test_qs, \"[Written by MAL Rewrite]\")\n",
    "\n",
    "processor = TextPreprocessor(VOCAB_SIZE)\n",
    "processor.create_tokenizer(train_qs)\n",
    "\n",
    "body_train = processor.transform_text(train_qs)\n",
    "body_test = processor.transform_text(test_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Preview the first input from our training data\n",
    "print(len(body_train[0]))\n",
    "print(body_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processor state of the tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('./processor_state.pkl', 'wb') as f:\n",
    "  pickle.dump(processor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_genres):\n",
    "  \n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(units=50, input_shape=(VOCAB_SIZE,), activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=25, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=num_genres, activation='sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_187 (Dense)           (None, 50)                45050     \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 76)                1976      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,301\n",
      "Trainable params: 48,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "19209\n",
      "Epoch 1/10\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.1690 - accuracy: 0.2401 - val_loss: 0.1092 - val_accuracy: 0.3285\n",
      "Epoch 2/10\n",
      "541/541 [==============================] - 1s 992us/step - loss: 0.1030 - accuracy: 0.3459 - val_loss: 0.1032 - val_accuracy: 0.3352\n",
      "Epoch 3/10\n",
      "541/541 [==============================] - 1s 942us/step - loss: 0.0968 - accuracy: 0.3602 - val_loss: 0.1002 - val_accuracy: 0.3628\n",
      "Epoch 4/10\n",
      "541/541 [==============================] - 1s 952us/step - loss: 0.0927 - accuracy: 0.3651 - val_loss: 0.0987 - val_accuracy: 0.3503\n",
      "Epoch 5/10\n",
      "541/541 [==============================] - 1s 986us/step - loss: 0.0898 - accuracy: 0.3741 - val_loss: 0.0984 - val_accuracy: 0.3524\n",
      "Epoch 6/10\n",
      "541/541 [==============================] - 0s 909us/step - loss: 0.0872 - accuracy: 0.3780 - val_loss: 0.0984 - val_accuracy: 0.3566\n",
      "Epoch 7/10\n",
      "541/541 [==============================] - 1s 925us/step - loss: 0.0850 - accuracy: 0.3894 - val_loss: 0.0987 - val_accuracy: 0.3431\n",
      "Epoch 8/10\n",
      "541/541 [==============================] - 0s 913us/step - loss: 0.0831 - accuracy: 0.3955 - val_loss: 0.0992 - val_accuracy: 0.3352\n",
      "Epoch 9/10\n",
      "541/541 [==============================] - 1s 928us/step - loss: 0.0812 - accuracy: 0.3985 - val_loss: 0.0995 - val_accuracy: 0.3550\n",
      "Epoch 10/10\n",
      "541/541 [==============================] - 0s 911us/step - loss: 0.0797 - accuracy: 0.4067 - val_loss: 0.1004 - val_accuracy: 0.3451\n",
      "151/151 [==============================] - 0s 535us/step - loss: 0.0970 - accuracy: 0.3450\n",
      "Eval loss/accuracy:[0.09703369438648224, 0.3449927270412445]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(VOCAB_SIZE, num_genres)\n",
    "model.summary()\n",
    "print(len(body_train))\n",
    "# Train and evaluate the model\n",
    "model.fit(body_train, train_genres, epochs=10, batch_size=32, validation_split=0.1)\n",
    "print('Eval loss/accuracy:{}'.format(\n",
    "  model.evaluate(body_test, test_genres, batch_size=32)))\n",
    "\n",
    "# Export the model to a file\n",
    "model.save('keras_saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prediction.py\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CustomModelPrediction(object):\n",
    "\n",
    "  def __init__(self, model, processor):\n",
    "    self._model = model\n",
    "    self._processor = processor\n",
    "  \n",
    "  def predict(self, instances, **kwargs):\n",
    "    preprocessed_data = self._processor.transform_text(instances)\n",
    "    predictions = self._model.predict(preprocessed_data)\n",
    "    return predictions.tolist()\n",
    "\n",
    "  @classmethod\n",
    "  def from_path(cls, model_dir):\n",
    "    import tensorflow.keras as keras\n",
    "    model = keras.models.load_model(\n",
    "      os.path.join(model_dir,'keras_saved_model.h5'))\n",
    "    with open(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
    "      processor = pickle.load(f)\n",
    "\n",
    "    return cls(model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_requests = [\"Despite being kind and considerate, Keitarou Itsuki has a menacing look in his eyes that scares away others. When Keitarou attempts to confess to Aoi Tokujira, she flatly rejects him, leading him to believe that his intimidating eyes are to blame. Confiding his worries to his older sister, she gives him a hand by offering a complete makeover. But much to Keitarou's surprise, she turns him into a girl! Forced to head home in his new appearance, he unexpectedly comes across Aoi being harassed by a group of boys from their school, and rushes to her rescue. Surprisingly, she doesn't recognize him, and reveals a secret that she has held for a long time: she has an intense fear of men. Believing him to be a tomboyish girl, she asks Keitarou in his female guise to assist her in overcoming her fear.\",\n",
    "                 \"Kusunoki used to believe he was destined for great things. Ostracized as a child, he held on to a belief that a good life was waiting for him in the years ahead. Now approaching the age of twenty, he's a completely mediocre college student with no motivation, no dreams, and no money. After learning he can sell his remaining years—and just how little they're worth—he chooses to divest himself of all but his last three months. Has Kusunoki truly destroyed his last chance to find happiness...or has he somehow found it?\",\n",
    "                 \"Helpless and struggling for cash, 20-year-old Kusunoki sells the last of his possessions to buy food. Noticing his poverty, an old shop owner directs him to a store that supposedly purchases lifespan, time, and health. While not completely believing the man's words, Kusunoki nevertheless finds himself at the address out of desperation and curiosity. Kusunoki is crushed when he finds out the true monetary value of his lifespan—totaling a meager three hundred thousand yen. Deciding to sell the next 30 years of his life for ten thousand yen per year, Kusunoki is left with only three months to live. After heading home with the money, he is greeted by an unexpected visitor: the same store clerk he sold his lifespan to. She introduces herself as Miyagi, the one tasked with the job of observing him until the last three days of his life. Jumyou wo Kaitotte Moratta. Ichinen ni Tsuki, Ichimanen de. follows the remaining three months of Kusunoki's life as he confronts lingering regrets from the past and discovers what truly gives life value.\",\n",
    "                 \"Shion and Rui are the dream team when it comes to hitting on women. Tonight was going to be another night of hooking up with girls for Shion, but he ended up taking a strange drug. When he woke up... he'd turned into a girl?! Rui came looking for Shion, but didn't recognize him, and started hitting on him...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.028682438656687737, 0.004946242086589336, 0.004648186266422272, 0.001957339234650135, 0.0009801577543839812, 0.00024222316278610379, 0.03139537572860718, 0.0015035879332572222, 0.005666667129844427, 8.708362292964011e-05, 0.18622522056102753, 0.008967697620391846, 0.004541065078228712, 0.0030598859302699566, 0.1774674952030182, 0.05387292429804802, 0.0003513977280817926, 0.025929395109415054, 0.029037175700068474, 0.017344282940030098, 0.0445873998105526, 0.001290512620471418, 0.008692910894751549, 0.052342288196086884, 0.8583765625953674, 0.0022367341443896294, 0.004768757149577141, 0.02067689411342144, 0.0005320966592989862, 8.626159251434729e-05, 0.0007426352822221816, 0.0017808370757848024, 0.027229048311710358, 0.010400562547147274, 0.010620174929499626, 0.0005447675357572734, 0.005568406078964472, 0.0005362792289815843, 0.0007668640464544296, 0.00024601133191026747, 0.003402964910492301, 0.008836920373141766, 0.030724642798304558, 0.008686971850693226, 0.003107145195826888, 0.01045361626893282, 0.004694919567555189, 0.0027215275913476944, 0.0007002211641520262, 0.008731567300856113, 0.00027235501329414546, 0.00046684607514180243, 0.00620714295655489, 0.21348409354686737, 0.004037066828459501, 0.00023803689691703767, 0.20022693276405334, 0.009463110007345676, 0.05111166089773178, 0.058522168546915054, 0.005387717857956886, 0.002093610353767872, 0.04754899442195892, 0.0014481761027127504, 0.004927733447402716, 0.0005678499001078308, 0.003585068741813302, 0.08282487094402313, 0.0007120661903172731, 0.013922304846346378, 0.0018027174519374967, 0.0019337751436978579, 0.009160131216049194, 0.0014947740128263831, 0.00096559536177665, 0.02429177798330784], [0.0035105275455862284, 0.00033672142308205366, 0.002212893683463335, 0.019658297300338745, 0.0006956625729799271, 0.00017441991076339036, 0.020327715203166008, 0.00013510377903003246, 0.0056313383392989635, 0.0007049341220408678, 0.0015844799345359206, 0.001505069201812148, 2.955917989311274e-05, 1.96847140614409e-05, 0.8112663626670837, 3.4678705560509115e-05, 0.26442620158195496, 0.0004370301612652838, 0.0073701259680092335, 1.9637782315840013e-05, 9.41092730499804e-05, 3.42666644428391e-05, 0.0024065033067017794, 5.7088371249847114e-05, 0.0038267713971436024, 8.658556907903403e-06, 0.35142216086387634, 0.0018749322043731809, 0.00254509667865932, 0.005720493383705616, 1.0144115549337585e-05, 0.004214453510940075, 0.0006672919844277203, 0.9152336120605469, 4.268941847840324e-05, 4.015427748527145e-06, 6.120331090642139e-05, 0.0025748719926923513, 8.919848914956674e-05, 0.0008920475956983864, 0.0015353267081081867, 0.6696065664291382, 0.0047604674473404884, 0.0009711190359666944, 0.004300083499401808, 0.00018517118587624282, 9.832673640630674e-06, 0.014925441704690456, 0.016664031893014908, 0.001299613737501204, 0.0009787207236513495, 4.6259770897449926e-05, 5.648403748637065e-05, 0.0073948269709944725, 0.0002707898383960128, 0.004773849155753851, 0.010586569085717201, 0.003064170014113188, 0.018621498718857765, 0.0011498250532895327, 0.0005872204783372581, 0.0053083342500030994, 0.22728873789310455, 0.00024566936190240085, 0.02653994783759117, 0.00029679323779419065, 0.0005088882171548903, 0.042510807514190674, 4.706302206614055e-06, 0.018767494708299637, 0.0009602857171557844, 0.0002560474385973066, 0.0006092931143939495, 0.00012974029232282192, 0.006962476298213005, 0.00031837570713832974], [0.0498899482190609, 0.016730209812521935, 0.03810678422451019, 0.006193973124027252, 0.003618216374889016, 0.0013781617162749171, 0.01885692961513996, 0.00862216018140316, 0.047639571130275726, 0.02342318929731846, 0.5051348805427551, 0.011402788572013378, 0.003948442172259092, 0.00081477384082973, 0.5107389092445374, 0.009031351655721664, 0.0006395644159056246, 0.006456111092120409, 0.0626625269651413, 0.014980240724980831, 0.002389471512287855, 0.0030315755866467953, 0.021401885896921158, 0.012885630130767822, 0.003191695548593998, 0.00022513647854793817, 0.45474281907081604, 0.01810171641409397, 0.004788147751241922, 0.001122647663578391, 0.001128370757214725, 0.03503217548131943, 0.027418343350291252, 0.01870190165936947, 0.012173628434538841, 0.004625690169632435, 0.0030936498660594225, 0.04550933837890625, 0.0028582378290593624, 0.005656600464135408, 0.010299455374479294, 0.020057180896401405, 0.03305963799357414, 0.02302306890487671, 0.01856209523975849, 0.01013115793466568, 0.039362430572509766, 0.006182847078889608, 0.051902785897254944, 0.005226000212132931, 0.0034315488301217556, 0.005495976656675339, 0.0073406025767326355, 0.23455531895160675, 0.00426111277192831, 0.043946582823991776, 0.011446076445281506, 0.013592170551419258, 0.18463346362113953, 0.07509037852287292, 0.12165874242782593, 0.013807475566864014, 0.3855660557746887, 0.0009085684432648122, 0.055442892014980316, 0.0016714065568521619, 0.015148140490055084, 0.07236059010028839, 0.0006692869937978685, 0.008081989362835884, 0.018236251547932625, 0.00663140881806612, 0.006816884037107229, 0.0036808897275477648, 0.024660464376211166, 0.03787672892212868], [0.1218123808503151, 0.027074692770838737, 0.05208524316549301, 0.007395302876830101, 0.0007578333606943488, 0.0012696414487436414, 0.08420301973819733, 0.008534431457519531, 0.008796711452305317, 0.0010280311107635498, 0.46567660570144653, 0.01591692864894867, 0.014476921409368515, 0.04008746147155762, 0.040767449885606766, 0.23775406181812286, 0.00041294770198874176, 0.036532700061798096, 0.09888831526041031, 0.023322617635130882, 0.10100524127483368, 0.011943957768380642, 0.019993256777524948, 0.46974605321884155, 0.7643217444419861, 0.007973561994731426, 0.0028662646654993296, 0.041074272245168686, 0.0001357692526653409, 0.00012580235488712788, 0.007852979935705662, 0.019714150577783585, 0.01954418420791626, 0.009115622378885746, 0.006430606823414564, 0.005350704304873943, 0.0028558552730828524, 0.005546420346945524, 0.030251914635300636, 0.0005914188222959638, 0.015380145981907845, 0.0024208046961575747, 0.03767463192343712, 0.037848372012376785, 0.003168549155816436, 0.0166301392018795, 0.006397353485226631, 0.002970441011711955, 0.0008989747730083764, 0.016136005520820618, 0.004300784319639206, 0.004052489995956421, 0.0027438667602837086, 0.3113904595375061, 0.04252875968813896, 0.0007974037434905767, 0.10089698433876038, 0.07037076354026794, 0.15544751286506653, 0.017420539632439613, 0.04413373023271561, 0.0017981089185923338, 0.04969297721982002, 0.002462042961269617, 0.002827097661793232, 0.004430260043591261, 0.008489681407809258, 0.13493432104587555, 0.0070410617627203465, 0.03390098735690117, 0.0011486854637041688, 0.004405630752444267, 0.02866167575120926, 0.007308719679713249, 0.0011674088891595602, 0.04043611139059067]]\n",
      "Predicted genres:\n",
      "Hentai\n",
      "\n",
      "\n",
      "Predicted genres:\n",
      "Drama\n",
      "Kids\n",
      "\n",
      "\n",
      "Predicted genres:\n",
      "\n",
      "\n",
      "Predicted genres:\n",
      "Hentai\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model_prediction import CustomModelPrediction\n",
    "\n",
    "classifier = CustomModelPrediction.from_path('.')\n",
    "results = classifier.predict(test_requests)\n",
    "print(results)\n",
    "\n",
    "for i in range(len(results)):\n",
    "  print('Predicted genres:')\n",
    "  for idx,val in enumerate(results[i]):\n",
    "    if val > 0.7:\n",
    "      print(genre_encoder.classes_[idx])\n",
    "  print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
